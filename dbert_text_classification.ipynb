{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faridelya/Deep-Learning/blob/main/dbert_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmNElBaBn2Kf"
      },
      "source": [
        "### Import all packages ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXV56P2Bn2Ki"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense,Dropout, Input\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix,f1_score,classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras import regularizers\n",
        "from transformers import *\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wapYawon2Kl"
      },
      "source": [
        "###  Preprocessing and cleaning functions  ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_YtxP82n2Kl"
      },
      "outputs": [],
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def clean_stopwords_shortwords(w):\n",
        "    stopwords_list=stopwords.words('english')\n",
        "    words = w.split() \n",
        "    clean_words = [word for word in words if (word not in stopwords_list) and len(word) > 2]\n",
        "    return \" \".join(clean_words) \n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    w=clean_stopwords_shortwords(w)\n",
        "    w=re.sub(r'@\\w+', '',w)\n",
        "    return w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAkdds2qn2Km"
      },
      "source": [
        "###  Reading and Cleaning the Dataset  ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DwPqjhmn2Kn",
        "outputId": "75ffc7bb-5f58-486b-f703-80b288f721f7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     v1                                                 v2 Unnamed: 2  \\\n",
              "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
              "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
              "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
              "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
              "\n",
              "  Unnamed: 3 Unnamed: 4  \n",
              "0        NaN        NaN  \n",
              "1        NaN        NaN  \n",
              "2        NaN        NaN  \n",
              "3        NaN        NaN  \n",
              "4        NaN        NaN  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_file='./data/spam.csv'\n",
        "data=pd.read_csv(data_file,encoding='ISO-8859-1')\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9N9f09Bn2Ko"
      },
      "source": [
        ">  Removing Unnamed Columns, dropping NaN data and resetting the index after dropping some rows/columns containing NaN dataset and finally shuffling the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ1f4eN8n2Kp",
        "outputId": "51ed8c53-18ad-478c-c828-e164cb31ebd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File has 5572 rows and 2 columns\n",
            "File has 5572 rows and 2 columns\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2011</th>\n",
              "      <td>ham</td>\n",
              "      <td>Do whatever you want. You know what the rules ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>ham</td>\n",
              "      <td>Say this slowly.? GOD,I LOVE YOU &amp;amp; I NEED ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>spam</td>\n",
              "      <td>4mths half price Orange line rental &amp; latest c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1938</th>\n",
              "      <td>ham</td>\n",
              "      <td>Excellent! Are you ready to moan and scream in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1903</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a weekly comp for a chance to ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        v1                                                 v2\n",
              "2011   ham  Do whatever you want. You know what the rules ...\n",
              "2763   ham  Say this slowly.? GOD,I LOVE YOU &amp; I NEED ...\n",
              "388   spam  4mths half price Orange line rental & latest c...\n",
              "1938   ham  Excellent! Are you ready to moan and scream in...\n",
              "1903  spam  Free entry in 2 a weekly comp for a chance to ..."
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = data.loc[:, ~data.columns.str.contains('Unnamed: 2', case=False)] \n",
        "data = data.loc[:, ~data.columns.str.contains('Unnamed: 3', case=False)] \n",
        "data = data.loc[:, ~data.columns.str.contains('Unnamed: 4', case=False)] \n",
        "print('File has {} rows and {} columns'.format(data.shape[0],data.shape[1]))\n",
        "data=data.dropna()\n",
        "data=data.reset_index(drop=True)\n",
        "print('File has {} rows and {} columns'.format(data.shape[0],data.shape[1]))\n",
        "data = shuffle(data)\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHCyQWK0n2Kq"
      },
      "source": [
        " > Rename v1:label and v2:text, converting 'ham' labels to '0' and 'spam' to '1', saving it to the 'gt' (ground truth) column and applying the preprocess function to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAzQAHpGn2Kr",
        "outputId": "6f2ad943-3da4-49d9-a286-306b05cbfbde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available labels:  ['ham' 'spam']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>gt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2011</th>\n",
              "      <td>ham</td>\n",
              "      <td>whatever want know rules talk earlier week sta...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>ham</td>\n",
              "      <td>say slowly god love amp need clean heart blood...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>spam</td>\n",
              "      <td>mths half price orange line rental latest came...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1938</th>\n",
              "      <td>ham</td>\n",
              "      <td>excellent ready moan scream ecstasy</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1903</th>\n",
              "      <td>spam</td>\n",
              "      <td>free entry weekly comp chance win ipod txt pod...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     label                                               text  gt\n",
              "2011   ham  whatever want know rules talk earlier week sta...   0\n",
              "2763   ham  say slowly god love amp need clean heart blood...   0\n",
              "388   spam  mths half price orange line rental latest came...   1\n",
              "1938   ham                excellent ready moan scream ecstasy   0\n",
              "1903  spam  free entry weekly comp chance win ipod txt pod...   1"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data=data.rename(columns = {'v1': 'label', 'v2': 'text'}, inplace = False)\n",
        "\n",
        "data['gt'] = data['label'].map({'ham':0,'spam':1})\n",
        "\n",
        "print('Available labels: ',data.label.unique())\n",
        "data['text']=data['text'].map(preprocess_sentence)\n",
        "\n",
        "num_classes=len(data.label.unique())\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Oj_M8hn2Ks"
      },
      "source": [
        "###  Loading DistilBERT Tokenizer and the DistilBERT model  ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnmm2rAfn2Ks"
      },
      "outputs": [],
      "source": [
        "dbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjjdC-8ln2Kt"
      },
      "outputs": [],
      "source": [
        "dbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyOpiE9Ln2Kt"
      },
      "source": [
        "###  Preparing input for the model  ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bWWSdyan2Ku",
        "outputId": "680b5195-c6a3-4ace-9212-879871f74415"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5572, 5572)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_len=32\n",
        "sentences=data['text']\n",
        "labels=data['gt']\n",
        "len(sentences),len(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1XGiKmDn2Ku"
      },
      "source": [
        "####  Let's take a sentence from the dataset and understand the input and output of the DistilBERT  #### "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJDCWrKYn2Kv"
      },
      "source": [
        "> Tokenized sentence "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11mtoKUCn2Kv",
        "outputId": "68c86468-3d3c-4789-ac49-dde7ffb58b4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ju',\n",
              " '##rong',\n",
              " 'point',\n",
              " 'crazy',\n",
              " 'available',\n",
              " 'bug',\n",
              " '##is',\n",
              " 'great',\n",
              " 'world',\n",
              " 'buffet',\n",
              " 'ci',\n",
              " '##ne',\n",
              " 'got',\n",
              " 'amore',\n",
              " 'wat']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dbert_tokenizer.tokenize(sentences[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4x0fWy6n2Kw"
      },
      "source": [
        "> Input ids and the attention masks from the tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyCGJuQun2Kw",
        "outputId": "32f189d7-839f-42b5-f8dd-b89052e448fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101,\n",
              "  18414,\n",
              "  17583,\n",
              "  2391,\n",
              "  4689,\n",
              "  2800,\n",
              "  11829,\n",
              "  2483,\n",
              "  2307,\n",
              "  2088,\n",
              "  28305,\n",
              "  25022,\n",
              "  2638,\n",
              "  2288,\n",
              "  26297,\n",
              "  28194,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0]}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dbert_inp=dbert_tokenizer.encode_plus(sentences[0],add_special_tokens = True,max_length =20,pad_to_max_length = True,truncation=True)\n",
        "dbert_inp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeDWtAtXn2Kw",
        "outputId": "e10bb2d3-fc0f-4652-a188-2ef16a0255c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[101,\n",
              " 18414,\n",
              " 17583,\n",
              " 2391,\n",
              " 4689,\n",
              " 2800,\n",
              " 11829,\n",
              " 2483,\n",
              " 2307,\n",
              " 2088,\n",
              " 28305,\n",
              " 25022,\n",
              " 2638,\n",
              " 2288,\n",
              " 26297,\n",
              " 28194,\n",
              " 102,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dbert_inp['input_ids']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-rh9EeXn2Kx"
      },
      "source": [
        ">  DistilBERT model output: Give input_ids and the attention_mask obtained from the tokenizer. The output will be a tuple of the size (1,max_len,768)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAQXzxcBn2Ky",
        "outputId": "6bfeb451-8392-4244-817f-7b454ae0b8fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tuple,\n",
              " (<tf.Tensor: shape=(1, 20, 768), dtype=float32, numpy=\n",
              "  array([[[ 0.603038  , -0.87843955, -0.27702317, ...,  0.34013888,\n",
              "           -0.31951576, -0.02768148],\n",
              "          [ 0.8059453 , -1.2426811 , -0.3692848 , ..., -0.00915013,\n",
              "           -0.2044661 , -0.12683335],\n",
              "          [ 0.7864537 , -0.9070081 , -0.44475678, ..., -0.00204397,\n",
              "           -0.31890398, -0.23745532],\n",
              "          ...,\n",
              "          [ 0.56349653, -1.0353185 , -0.26982975, ...,  0.37219822,\n",
              "           -0.30490598, -0.09034443],\n",
              "          [ 0.5596978 , -1.0259491 , -0.29068822, ...,  0.33829993,\n",
              "           -0.29505792, -0.1154284 ],\n",
              "          [ 0.70843756, -0.9754098 , -0.21341297, ...,  0.42030877,\n",
              "           -0.3028642 , -0.06653835]]], dtype=float32)>,))"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id_inp=np.asarray(dbert_inp['input_ids'])\n",
        "mask_inp=np.asarray(dbert_inp['attention_mask'])\n",
        "out=dbert_model([id_inp.reshape(1,-1),mask_inp.reshape(1,-1)])\n",
        "type(out),out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVffvCMOn2Ky"
      },
      "source": [
        "> Obtain the embeddings of a sentence from the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Da3s-gvn2Kz"
      },
      "outputs": [],
      "source": [
        "out[0][:,0,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSuDMEnZn2Kz"
      },
      "source": [
        "> Decode the original sentence from the tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAL78pN4n2K0",
        "outputId": "238117e2-bc75-46f2-c01b-d51ecd8fec63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[CLS] jurong point crazy available bugis great world buffet cine got amore wat [SEP] [PAD] [PAD] [PAD]'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dbert_tokenizer.decode(dbert_inp['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n9cbUYVn2K0"
      },
      "source": [
        "###  Create a basic NN model using DistilBERT embeddings to get the predictions  ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tD4gysEPn2K0"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    inps = Input(shape = (max_len,), dtype='int64')\n",
        "    masks= Input(shape = (max_len,), dtype='int64')\n",
        "    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n",
        "    dense = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n",
        "    dropout= Dropout(0.5)(dense)\n",
        "    pred = Dense(num_classes, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n",
        "    print(model.summary())\n",
        "    return model   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiZBDtP7n2K0"
      },
      "source": [
        "> Feel free to add more Dense and Dropout layers with variable units and the regularizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrmVzVw_n2K1",
        "outputId": "e7fbd203-1241-411d-e568-3a86e345624c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <bound method TFDistilBertModel.call of <transformers.modeling_tf_distilbert.TFDistilBertModel object at 0x7fdff6ca9490>> and will run it as-is.\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 32)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model_4 (TFDisti ((None, 32, 768),)   66362880    input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [(None, 768)]        0           tf_distil_bert_model_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          393728      tf_op_layer_strided_slice[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_95 (Dropout)            (None, 512)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            1026        dropout_95[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 66,757,634\n",
            "Trainable params: 66,757,634\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model=create_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA6xT7LYn2K1"
      },
      "source": [
        "> Prepare the model input "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE4TZcPwn2K1"
      },
      "outputs": [],
      "source": [
        "input_ids=[]\n",
        "attention_masks=[]\n",
        "\n",
        "for sent in sentences:\n",
        "    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n",
        "    input_ids.append(dbert_inps['input_ids'])\n",
        "    attention_masks.append(dbert_inps['attention_mask'])\n",
        "\n",
        "input_ids=np.asarray(input_ids)\n",
        "attention_masks=np.array(attention_masks)\n",
        "labels=np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE1bjbkSn2K1",
        "outputId": "11401f50-24ea-4048-b5f7-c14e4e774389"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5572, 5572, 5572)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(input_ids),len(attention_masks),len(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD81bUvin2K2"
      },
      "source": [
        "> Save the model input in the pickle files to use it later without performing the above steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqXFrha7n2K2",
        "outputId": "a363046d-c9a1-4404-c0f0-d25971a1ce34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing the pickle file.....\n",
            "Pickle files saved as  ./data/dbert_inp.pkl ./data/dbert_mask.pkl ./data/dbert_label.pkl\n"
          ]
        }
      ],
      "source": [
        "print('Preparing the pickle file.....')\n",
        "\n",
        "pickle_inp_path='./data/dbert_inp.pkl'\n",
        "pickle_mask_path='./data/dbert_mask.pkl'\n",
        "pickle_label_path='./data/dbert_label.pkl'\n",
        "\n",
        "pickle.dump((input_ids),open(pickle_inp_path,'wb'))\n",
        "pickle.dump((attention_masks),open(pickle_mask_path,'wb'))\n",
        "pickle.dump((labels),open(pickle_label_path,'wb'))\n",
        "\n",
        "\n",
        "print('Pickle files saved as ',pickle_inp_path,pickle_mask_path,pickle_label_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMOw3XMjn2K2",
        "outputId": "30dab770-89a3-4f9a-8474-24e8699cb7df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the saved pickle files..\n",
            "Input shape (5572, 32) Attention mask shape (5572, 32) Input label shape (5572,)\n"
          ]
        }
      ],
      "source": [
        "print('Loading the saved pickle files..')\n",
        "\n",
        "input_ids=pickle.load(open(pickle_inp_path, 'rb'))\n",
        "attention_masks=pickle.load(open(pickle_mask_path, 'rb'))\n",
        "labels=pickle.load(open(pickle_label_path, 'rb'))\n",
        "\n",
        "print('Input shape {} Attention mask shape {} Input label shape {}'.format(input_ids.shape,attention_masks.shape,labels.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-jd5sZGn2K3"
      },
      "outputs": [],
      "source": [
        "label_class_dict={0:'ham',1:'spam'}\n",
        "target_names=label_class_dict.values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J68h6Bj5n2K3"
      },
      "source": [
        "> Train Test split and setting up the loss function, accuracy and optimizer for the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CctjqScAn2K3",
        "outputId": "4b0f62b7-91f6-4593-de05-27dd7c842bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train inp shape (4457, 32) Val input shape (1115, 32)\n",
            "Train label shape (4457,) Val label shape (1115,)\n",
            "Train attention mask shape (4457, 32) Val attention mask shape (1115, 32)\n"
          ]
        }
      ],
      "source": [
        "train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.2)\n",
        "\n",
        "print('Train inp shape {} Val input shape {}\\nTrain label shape {} Val label shape {}\\nTrain attention mask shape {} Val attention mask shape {}'.format(train_inp.shape,val_inp.shape,train_label.shape,val_label.shape,train_mask.shape,val_mask.shape))\n",
        "\n",
        "\n",
        "log_dir='dbert_model'\n",
        "model_save_path='./dbert_model.h5'\n",
        "\n",
        "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "model.compile(loss=loss,optimizer=optimizer, metrics=[metric])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HWko9fyn2K3"
      },
      "outputs": [],
      "source": [
        "callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "model.compile(loss=loss,optimizer=optimizer, metrics=[metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2olEKbzn2K4"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "jwC26Bsin2K4",
        "outputId": "effa2c5c-cefb-4335-8339-a6f7204eb028"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1/279 [..............................] - ETA: 0s - loss: 6.7663 - accuracy: 0.8750\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "279/279 [==============================] - ETA: 0s - loss: 5.6354 - accuracy: 0.9435\n",
            "279/279 [==============================] - 251s 898ms/step - loss: 5.6354 - accuracy: 0.9435 - val_loss: 4.7500 - val_accuracy: 0.9749\n",
            "Epoch 2/5\n",
            "279/279 [==============================] - 244s 875ms/step - loss: 4.0501 - accuracy: 0.9749 - val_loss: 3.4032 - val_accuracy: 0.9821\n",
            "Epoch 3/5\n",
            "279/279 [==============================] - 247s 884ms/step - loss: 2.8908 - accuracy: 0.9796 - val_loss: 2.4331 - val_accuracy: 0.9704\n",
            "Epoch 4/5\n",
            "279/279 [==============================] - 249s 894ms/step - loss: 2.0531 - accuracy: 0.9818 - val_loss: 1.7228 - val_accuracy: 0.9794\n",
            "Epoch 5/5\n",
            "279/279 [==============================] - 244s 874ms/step - loss: 1.4562 - accuracy: 0.9877 - val_loss: 1.2264 - val_accuracy: 0.9857\n"
          ]
        }
      ],
      "source": [
        "history=model.fit([train_inp,train_mask],train_label,batch_size=16,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlYci_8En2K4"
      },
      "source": [
        "### Tensorboard visualization (Training-Testing curve) ###\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z4RBjegn2K5"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqQ031e2n2K5",
        "outputId": "a246e24a-223c-4126-f6ec-82c5d26be8b1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-1d79222d114abbf0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-1d79222d114abbf0\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6007;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%tensorboard --logdir {log_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiValWwCn2K6"
      },
      "source": [
        "< Increase the number of epochs in order to decrease the loss further"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dUSqAjvn2K6"
      },
      "source": [
        "### Use the saved model for predictions and calculating the evaluation metrics ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhNLqkTun2K6",
        "outputId": "3c5bd6be-efc8-49d9-f749-94e91e28da8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"functional_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 32)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 32)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model_4 (TFDisti ((None, 32, 768),)   66362880    input_5[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_2 (Te [(None, 768)]        0           tf_distil_bert_model_4[2][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 512)          393728      tf_op_layer_strided_slice_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dropout_97 (Dropout)            (None, 512)          0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 2)            1026        dropout_97[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 66,757,634\n",
            "Trainable params: 66,757,634\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "trained_model = create_model()\n",
        "trained_model.compile(loss=loss,optimizer=optimizer, metrics=[metric])\n",
        "trained_model.load_weights(model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dS4es7Rsn2K7",
        "outputId": "d0aea09c-7bba-431f-843d-f609156b28f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fdf88126e60> and will run it as-is.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9424460431654677"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds = trained_model.predict([val_inp,val_mask],batch_size=16)\n",
        "pred_labels = preds.argmax(axis=1)\n",
        "f1 = f1_score(val_label,pred_labels)\n",
        "f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oCYRyMyn2K7",
        "outputId": "0b24c6df-2677-42a2-815b-8e7124750182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score 0.9424460431654677\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.99      0.99       973\n",
            "        spam       0.96      0.92      0.94       142\n",
            "\n",
            "    accuracy                           0.99      1115\n",
            "   macro avg       0.98      0.96      0.97      1115\n",
            "weighted avg       0.99      0.99      0.99      1115\n",
            "\n",
            "Training and saving built model.....\n"
          ]
        }
      ],
      "source": [
        "target_names=['ham','spam']\n",
        "print('F1 score',f1)\n",
        "print('Classification Report')\n",
        "print(classification_report(val_label,pred_labels,target_names=target_names))\n",
        "\n",
        "print('Training and saving built model.....')   "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}